---
description: Data Infrastructure
---

# 1.X 데이터 파이프라인

)이번 챕터에서는 데이터 파이프라인의 구성요소에 대해 이야기 합니다. 가상의 작은 회사 우동마켓을 가정하고, 우동마켓이 성장하면서 마주치는 문제들을 데이터 인프라 컴포넌트와 함께 하나씩 알아봅니다.

{% hint style="info" %}
우동마켓은 장류진 작가님의 소설 [일의 기쁨과 슬픔](http://www.yes24.com/Product/Goods/80742923) 속에 나온 가상의 스타트업입니다. 현실 세계에 존재하는 회사와는 관련이 없습니다.&#x20;
{% endhint %}



가상의 스타트업 우동마켓은 글로벌한 우동 서비스를 제공하려는 야욕을 가지고 있습니다. 면과 토핑, 국물 및 송송썰린 파와 같은 재료의 직판매 부터 시작해서 수 많은 우동 매니아를 위한 우동 추천 서비스, 비행기 입국장에서 사용자를 노려 국내 유수의 우동집을 홍보하는 타게팅까지를 목표로 하고 있습니다.&#x20;

이른바 "우동 커머스" 를 제공하려는, AWS 를 사용하는 초기 단계의 우동 스타트업의 데이터 파이프라인이 어떨지 논의해 봅시다.



### 초기 단계&#x20;

초기 단계의 스타트업에는 데이터 엔지니어가 필요 없을지 모릅니다.

* AWS 내에 [RDS](https://aws.amazon.com/rds/?p=pm\&c=db\&z=3), Redis (세션 등 용도) 와 같은 필수 저장소만 사용하고 있고&#x20;
* 아직은 기능을 만드는데 바빠 상품 검색은 MySQL 에서 LIKE 검색을
* 추천 상품은 단순히 판매건 기준으로 정렬해 Top 10 만 내보내고 있습니다

\[그림] 우동마켓 초기 인프라 (AWS VPC 내 RDS, EC, EC2 정도)



이때 기획자로부터 첫 번째 데이터 요건이 들어옵니다.

* 데이터를 보고 싶다
* 실시간이면 좋겠다
* 개발자들은 기능 만드는데 바쁘니 쿼리보다는 UI 를 통해 이런저런 데이터를 보거나 가공할 수 있어야 한다



가장 쉬운 방법은 실제 서비스 DB 에 붙어 데이터를 보는것입니다. 물론 당연히 (...) 그러면 안됩니다. 따라서 AWS RDS 에 [Read Replica](https://aws.amazon.com/ko/rds/features/read-replicas/) 를 이용하면 (데이터가 적은 우동마켓의 경우에는) 아주 약간의 지연이 있을뿐, 실제 서비스 DB 를 실시간 / 비동기로 복제해 데이터를 확인할 수 있습니다.&#x20;

![AWS RDS 읽기 전용 복제본 (https://aws.amazon.com/ko/rds/features/read-replicas)](<../.gitbook/assets/image (2).png>)

[AWS Aurora](https://aws.amazon.com/ko/rds/aurora/?aurora-whats-new.sort-by=item.additionalFields.postDateTime\&aurora-whats-new.sort-order=desc) 를 RDB 로 사용한다면 내부적으로는 조금 다르게 동작할 수 있으나 마찬가지로 [Aurora Read Replica](https://docs.aws.amazon.com/ko\_kr/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html) 를 추가할 수 있습니다.&#x20;

* AWS Aurora 는 최대 15개의 Read Replica 를 지원합니다.
* AWS RDS 는 최대 5개의 Read Replica 를 지원합니다.

Aurora 와 RDS 는 내부적인 구조가 다릅니다. 다음 그림을 통해 동작 방식이 어떻게 다를지 추론해볼 수 있습니다.

![AWS RDS vs Aurora (https://aws.amazon.com/ko/blogs/database/is-amazon-rds-for-postgresql-or-amazon-aurora-postgresql-a-better-choice-for-me/)](<../.gitbook/assets/image (4) (1).png>)



이렇게 추가된 읽기 전용 DB 에 우동마켓 내부 직원들이 사용하기 편리한 데이터 탐색 도구를 선정해 붙일 수 있습니다.&#x20;

* [AWS Quicksight](https://docs.aws.amazon.com/ko\_kr/quicksight/latest/user/how-quicksight-works.html) 와 같은 클라우드 관리형 도구를 사용하거나
* [Metabase](https://www.metabase.com) 처럼 쉽게 드래그 & 드랍을 통해 누적 차트와 대시보드를 쉽게 만들 수 있는 오픈소스 도구를 활용하거나
* [Retool](https://retool.com) 과 같이 내부 사용자 Admin 을 개발 없이 빠르게 보는 도구를 사용할 수도 있습니다.
* [Redash](https://redash.io) 를 사용한다면 DB 뿐만 아니라 구글스프레드시트 등 다양한 도구와 연동되며, 내부 사용자들이 쿼리를 작성하고, 쉽게 공유하고 그것으로 대시보드를 만들 수 있습니다.
* 이외에도 [Superset](https://superset.apache.org) 이나 [Tableau](https://www.tableau.com/ko-kr) 와 같은 도구들은 데이터를 '한번 더' 가공해 깔끔하게 만든 뒤 대시보드에 다양한 기능을 넣고 꾸미는 기능에 강점이 있기도 합니다.

도구는 회사 내부 사용자들의 생산성을 결정하므로, 적절한 도구를 선택해 제공할 수 있습니다.&#x20;

일반적으로는 여러 종류의 DB (RDB, NoSQL) 는 물론 구글 시트 등 다양한 시스템과 연동되고 다른 사람이 작성한 쿼리 / 대시보드 등을 쉽게 검색하고 재활용할 수 있으며 운영성으로 대용량 CSV 다운로드가 가능한 Redash 가 많이 쓰이는 편입니다.&#x20;

필요하다면 여러 도구를 제공하는 것도 물론 가능합니다. 다만 늘 그렇듯이, 해당 시스템을 운영하는 엔지니어의 노동 비용 그리고 머신 값으로 지불해야 할 물리 비용, 사내 사용자들의 학습 부하 및 데이터 활용의 집중이 아니라 분산 (다양한 도구 사용시) 이 발생할 수 있으므로 적절한 한도 내에서 몇몇 도구를 사용할지 결정하면 됩니다.

{% hint style="info" %}
여러분이 도입하는 모든 데이터 도구는, 데이터로 일하는 회사 구성원 전부의 생산성에 영향을 미칩니다. 따라서 단순히 '이 기능이 좋아' 보다는 다음 내용들을 고려하여 제공해야 합니다.

* 여러 시스템과의 연동
* 코드 수정 및 확장 가능성
* 다양한 활용처: 대용량 CSV 데이터 다운로드 / 데이터 탐색 / 대시보드 등
{% endhint %}



\[그림] Read Replica 에 다양한 도구를 붙이는 경우



데이터 탐색 도구를 DB 에 붙이면 이제 준 실시간으로 (Read Replica 의 지연을 제외하면) 서비스의 데이터를 볼 수 있습니다. 그러나 몇 가지 제한점이 존재합니다.&#x20;

* 우동마켓은 그 자그마한 서비스 규모에 걸맞지 않게, MSA 를 하느라 DB 를 여러개 사용합니다. DB 가 여러개이므로 DB 내에서 Join 이 불가능합니다.
* 우동마켓은 RDB 이외에도 [Elasticache](https://aws.amazon.com/ko/elasticache/) (AWS managed ElasticSearch) 와 AWS OpenSearch (AWS 관리형 ElasticSearch) 를 사용합니다.&#x20;
* 우동마켓의 서비스 DB 인 RDB 는 사용할 수 있는 Query 문법이 별로 신통치 않습니다. MySQL 특정 버전 이후부터 With Clause Window Function, JSON Function, Spatial Function 이 지원됩니다. 다만 그정도이지, Unnest 등 다양한 패턴을 일관되고 사용성 높은 함수로 지원하지 못합니다. 게다가 문법이 다른 MySQL 이외의 DB 를 인수합병, 너와 내가 사이가 나빠 시작하는 MSA 등으로 도입하는 순간 쿼리 사용자는 지옥으로 가게됩니다.



따라서 다음의 요구사항을 충족하는 범용, 대규모 처리 쿼리 엔진이 필요합니다.

* 다양한 스토리지 (RDB, NoSQL) 를 지원하고 서로 다른 스토리지의 데이터를 Join 할 수 있습니다.
* 스토리지에 상관 없이 일관되고 사용성이 편리한 SQL 문법을 제공합니다.
* 대규모 데이터를 메모리에서 빠르게 처리할 수 있습니다.

[Apache Presto](1.1.md#undefined) 는 이런 용도를 위해 설계된 범용, 대규모 쿼리 엔진입니다. AWS 에서는 [EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-presto.html) 위에서 편리하게 사용할수 있습니다. 만약 직접 운영하지 않는다면 [AWS Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html) 와 같은 관리형 서비스를 사용할 수 있습니다. ([Athena 는 Presto 를 기반으로 만들어졌습니다.](https://aws.amazon.com/ko/athena/faqs/))

![Presto Connector Overview (https://prestodb.io/overview.html)](<../.gitbook/assets/image (1) (1).png>)



Presto 를 이용하면 다양한 커넥터를 붙여 SQL 을 이용해 Join 하고 가공할 수 있습니다. 예를 들어, 아래의 Presto 쿼리는 RDB 로는 처리하기 힘든 대량의 데이터를 보관하는 Hive (엄밀히는 S3 또는 HDFS) 와 실제 서비스에 내보내는 RDB Replica 내의 메타값을 Join 해 집계하는, 서로 다른 두 저장소를 Presto 내에서 묶어 컴퓨팅하는 예제입니다.

```sql
SELECT AIRPORT.name as airport_name, count(*) as flight_count
FROM hive.flight_history.flights FLIGHT
JOIN mysql.flight_meta.airport AIRPORT 
  ON FLIGHT.code = AIRPORT.airport_code
GROUP BY AIRPORT.name
ORDER BY flight_count
LIMIT 100
```

Presto 는 JDBC / ODBC 를 통해 사용이 가능하며, HTTP API 로도 쿼리를 실행할 수 있습니다. 위 Presto 설명에 나온 사진과 같이 대부분의 오픈소스 / 상용 데이터 분석 도구는 전부 사용이 가능하며, 필요시 내부에서 JDBC / HTTP 등의 프로토콜을 이용해 직접 인하우스 분석 도구를 개발하는 것도 가능합니다.



\[그림] Presto 또는 Athena 를 통한 우동마켓 서비스 분석도





### 중반부



이제 데이터를 '보는 것' 은 가능해 졌습니다. 원하는 스토리지에서 쿼리해 대시보드를 만들수도 있고, 마케팅이나 CS 대응과 같은 필요한 데이터도 CSV 로 추출하는것이 가능합니다.

쿼리를 모르는 사용자도, 자신이 사용하는 Google Sheet 를 분석 도구에 연동하고 [Widget](https://redash.io/help/user-guide/querying/query-parameters) 을 활용해 데이터 분석 마우스만으로도 운영성 데이터를 뽑아낼 수 있습니다.

![Redash Query Parameter (https://redash.io/help/user-guide/querying/query-parameters)](<../.gitbook/assets/image (5) (1).png>)



그러나 여전히 몇 가지 문제가 있습니다.

* Read Replica 를 통해 (준) 실시간으로 데이터를 확인하는건 좋은 일이지만, 컴퓨팅이 스토리지에 의존합니다. 다시 말해, Read Replica 사이즈에 따라 스캔 속도가 좌우됩니다.&#x20;
* Presto 가 [Predicate Push Down](https://trino.io/docs/current/optimizer/pushdown.html) 지원하지 않는 스토리지거나, 특정 Presto SQL 구문의 경우에만 Push Down 이 동작하지 않을수도 있습니다. Push Down 을 이용하면, 필터링을 데이터를 가져온 이후에 메모리에서 수행하는 것이 아니라 데이터를 가져오는 시점부터 저장소가 제공하는 필터링을 사용할 수 있기 때문에 훨씬 효율적입니다.
* 스토리지 종류에 따라 당장 Read Replica 를 추가하기 어려운 경우도 있습니다. 관리형 서비스인 AWS RDS, Aurora, Elasticache (Redis, 모드에 따라) 는 읽기 전용 Endpoint 를 지원하지만 다른 저장소는 그렇지 않을수도 있습니다.

또한 RDB 의 데이터는 무한히 적재할 수 없습니다. 주문, 예약 등 비즈니스 크리티컬한 데이터가 아니라 단순 History 성 테이블의 경우에는 어느 순간에는 특정 시점을 기준으로 과거 데이터는 서비스 테이블에서 분리해 보존처리 해야할 수 있습니다.

이런 이유로 인해 '주기적으로' 데이터를 대량의 컴퓨팅을 수행할 수 있는 저장소로 옮겨오는 작업이 필요하게 됩니다.&#x20;

* RDB, NoSQL (Redis, ElasticSearch 등) 에서 데이터를 읽어 AWS S3 / HDFS 등에 저장합니다.&#x20;
* HDFS 는 별도로 Hadoop 을 운영해야 하므로 특정 상황이 아니라면 적은 규모에서는 S3 에 데이터를 적재합니다.&#x20;
* AWS LB Acess Log / RDS Export 등 AWS 서비스는 S3 에 데이터를 저장하는 것을 지원합니다.&#x20;
* AWS S3 내 JSON, [Parquet](1.1.md#undefined) 등의 형식으로 적재시에 Athena 등 다양한 서비스에서 읽어 사용할 수 있습니다.&#x20;
* AWS S3 는 데이터를 저장하고 데이터를 읽는 용도 이외에도 [Storage Class](1.1.md#undefined) 를 통해 사용이 덜 되는 데이터는 Tier 를 낮추어 비용을 절감하는 등의 다양한 기능을 제공합니다.

[Apache Parquet](https://parquet.apache.org/documentation/latest/) 는 추후 File Format 챕터에서 다루기에 간단히만 설명하자면 다음과 같습니다.

* 컬럼을 중심으로 데이터를 저장하는 Columnar Format 입니다. 대부분의 데이터 조회 및 가공시에는 전체 컬럼을 접근하지 않고 (`SELECT *`), 일부 컬럼만을 사용하므로 (`SELECT airport_name, airport_code`) 데이터를 읽어오는 시점부터 필요한 컬럼만 읽어올 수 있습니다.
* 컬럼 중심으로 데이터를 저장하므로 성별이나 도시 등의 ENUM 값처럼 Cardinality 가 비교적 낮은 값들의 데이터 사이즈를 많이 줄일 수 있습니다. (e.g, MALE = 1, FEMALE = 2 로 저장) 또한 File 자체에서 타입과 스키마를 지원하므로 파일을 읽는 시점부터 마치 RDB 와 같은 저장소처럼 데이터를 필터링해 가져올 수 있습니다.&#x20;
* Snappy 등과 같은 압축 알고리즘과 같이 사용하면, 파일 사이즈를 굉장히 많이 줄일 수 있습니다.&#x20;



다음 사진은 AWS RDS 또는 AWS Aurora 에서 S3 로 DB 스냅샷을 내보내는 걸 도식화 한 그림입니다.

\[그림] RDS 데이터를 주기적으로 퍼오는 그림 S3

![RDB Replica Pattern ()https://aws.amazon.com/ko/blogs/database/building-data-lakes-and-implementing-data-retention-policies-with-amazon-rds-snapshot-export-to-amazon-s3/](<../.gitbook/assets/image (5).png>)

![S3 Export Pattern (https://aws.amazon.com/ko/blogs/database/building-data-lakes-and-implementing-data-retention-policies-with-amazon-rds-snapshot-export-to-amazon-s3/)](../.gitbook/assets/image.png)





한 가지 고민해보아야 할 점이 있습니다. AWS RDS / Aurora 를 S3 로 Snapshot 내보내는 AWS CLI API 를 살펴보면 어떤 인스턴스 내 DB / Table 를 뽑아낼지를 결정할 수 있습니다.

* [AWS RDS (Aurora) CLI start-export-task](https://docs.aws.amazon.com/cli/latest/reference/rds/start-export-task.html)
* [AWS RDS Export Snapshot Limitation](https://docs.aws.amazon.com/ko\_kr/AmazonRDS/latest/UserGuide/USER\_ExportSnapshot.html)

```
  start-export-task
--export-task-identifier <value>
--source-arn <value>
--s3-bucket-name <value>
--iam-role-arn <value>
--kms-key-id <value>
[--s3-prefix <value>]
[--export-only <value>]
[--cli-input-json <value>]
[--generate-cli-skeleton <value>]

```



CLI ㅇ

&#x20;Column 결정할 수 없음. 컴퓨팅 사이즈도 결정할 수 없음

따라서 단순 백업을 보관하는 용도에 가깝다는것을 알 수 있음. 컬럼 변환, 타입 지정 불가, 컴퓨팅 사이즈 불가

개인정보 존재시 S3 에 개인정보가 노출, 접근제어가 사용자 단위로 불가능



이보다 더 많은 수준의 제어를 제공하는 AWS DMS (Database Migration Service)

* 그러나 지정된 데이터 소스 및 타겟
  * [AWS DMS Available Source](https://docs.aws.amazon.com/dms/latest/userguide/CHAP\_Source.html)
  * [AWS DMA Available Target](https://docs.aws.amazon.com/dms/latest/userguide/CHAP\_Target.html)
* CDC 를 통해 준 실시간 복제 가능
* On-prem to AWS managed RDB
* AWS Specific Storage Migration (Dynamo)
* DB 입수에는 최적화&#x20;

Glue ETL 커스텀 코드 작성

EMR 클러스터 운영 및&#x20;









RDS Exporter

{% hint style="info" %}
RDS Exporter 가 원하는 기능을 처음부터 제공하지 않을수도

미래에도 원하는 기능을 제공하지 않을수도 주기 / 컴퓨팅 속도 / 추출 컬럼 컨버팅 등&#x20;

직접 만들어야?
{% endhint %}



&#x20;





Client (앱 / 웹) 의 사용자는 아직 존재하지 않기 때문에 볼 수 없습니다.&#x20;

\[그림] 사용자 데이터 시나리오&#x20;

* Kinesis
* API / Kafka



\[그림] 실시간 사용자 데이터 처리&#x20;



또한 서버로부터 실시간 로그

\[그림] 서버 실시간 데이터 처리



\[그림]&#x20;











