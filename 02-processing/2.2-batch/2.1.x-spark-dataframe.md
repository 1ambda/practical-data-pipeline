# 2.1.X Spark DataFrame

Spark Tutorial 에서는 DataFrame 의 기초적인 내용을 2천 건 정도의 작은 데이터셋을 이용해 다루어보았습니다. 이제는 조금 더 복잡하면서도 실제로 많이 활용되는 사례를 바탕으로 유용한 함수들을 알아봅시다.&#x20;

이번 챕터에서 사용할 데이터는 [Kaggle eCommerce Events History In Cosmetic Shop](https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop?select=2020-Jan.csv) (2020-Jan.csv) 입니다.

`ecommerce_event.csv` 란 이름으로 저장 후 다음과 같이 [SparkSession.read.load](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html) 함수를 이용해 로딩합니다. 이번에 사용하는 데이터셋의 구분자는 `\t` 가 아니라 `,` (comma) 이므로 `sep` 옵션을 생략해도 좋습니다.

```python
from pyspark.sql.types import *
from pyspark.sql.functions import *

df = spark.read.load("./ecommerce_event.csv",
                     format="csv", inferSchema="true", header="true")

df.count() # 4264752, 약 450 MiB 파일
df.printSchema()

root
 |-- event_time: string (nullable = true)
 |-- event_type: string (nullable = true)
 |-- product_id: integer (nullable = true)
 |-- category_id: long (nullable = true)
 |-- category_code: string (nullable = true)
 |-- brand: string (nullable = true)
 |-- price: double (nullable = true)
 |-- user_id: integer (nullable = true)
 |-- user_session: string (nullable = true)
```



### Basic & Aggregation

우선 `event_time` 컬럼을 살펴봅시다.  [agg](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.agg.html?highlight=agg) 함수는  SQL 에서 Group By 후 Select 에서 사용하는 Aggregation 함수와 동일합니다.

```python
df.agg(max("event_time"), min("event_time")).show(truncate=False)
```

```

+-----------------------+-----------------------+
|max(event_time)        |min(event_time)        |
+-----------------------+-----------------------+
|2020-01-31 23:59:58 UTC|2020-01-01 00:00:00 UTC|
+-----------------------+-----------------------+
```



`brand` 별로 `category` 를 살펴봅시다. SQL 로 치자면 Group By 후 Count 하는 간단한 함수입니다.

```python
df.groupBy(col("brand"), col("category")).agg(count("*")).show()

```

```
+---------+-------------------+--------+
|    brand|        category_id|count(1)|
+---------+-------------------+--------+
|   runail|1487580007936033754|    4395|
|     oniq|1487580005092295511|    3298|
|    domix|1487580011970953351|    1292|
|  bioaqua|1597770225539875791|     113|
|    domix|1487580007256556476|    1133|
| ingarden|1487580011996119176|     900|
|  markell|1783999067156644376|     205|
|     null|1487580008145748965|   19054|
|     null|1487580011677352062|    6008|
|     null|1998040852064109417|     880|
|beautific|1487580008288355308|      37|
|    naomi|1487580012524601496|      86|
|   missha|1783999073758478650|       6|
|     null|2145935122136826354|       1|
|   runail|1487580009051717646|    2630|
|     kiss|1487580013506068678|     237|
|      pnb|1487580007457883075|     491|
|bespecial|1487580013338296510|     134|
|   eunyul|1487580011585077370|    1860|
|   matrix|1487580008263189483|     573|
+---------+-------------------+--------+
```



{% hint style="info" %}
여러분이 작업하는 대부분의 DataFrame 데이터 가공은 SQL 로 생각하면 쉽습니다. 대응하는 함수가 거의 대부분 있습니다. 필요하다면 [Spark SQL](https://spark.apache.org/sql/) 을 사용할 수 있습니다.
{% endhint %}



```python
df\
    .groupBy("brand", "category_code")\
    .agg(countDistinct("product_id").alias("product_count"))\
    .show(truncate=False)
```

```

+------------+--------------------------------------+-------------+
|brand       |category_code                         |product_count|
+------------+--------------------------------------+-------------+
|beautix     |null                                  |316          |
|dr.gloderm  |null                                  |31           |
|farmona     |null                                  |39           |
|profhenna   |null                                  |56           |
|runail      |appliances.environment.vacuum         |3            |
|invisibobble|null                                  |2            |
|macadamia   |appliances.environment.air_conditioner|1            |
|riche       |null                                  |58           |
|nova        |null                                  |1            |
|oniq        |null                                  |590          |
|lebelage    |null                                  |45           |
|fancy       |null                                  |15           |
|vilenta     |null                                  |20           |
|siberina    |null                                  |181          |
|tertio      |null                                  |115          |
|jaguar      |null                                  |21           |
|nitrimax    |apparel.glove                         |21           |
|jas         |null                                  |16           |
|rocknailstar|null                                  |6            |
|koreatida   |null                                  |3            |
+------------+--------------------------------------+-------------+
```



SQL 과 유사하게 하나의 컬럼에 존재하는 ENUM 타입 값을 보기 위해 Group By 를 사용할수도 있겠지만, Distinct 를 활용할 수 있습니다.

```python
df.select("event_type").distinct().show()
```

```

+----------------+
|      event_type|
+----------------+
|        purchase|
|            view|
|            cart|
|remove_from_cart|
+----------------+
```



**purchase** (구매) 이벤트가 있으니, 일별로 구매 이벤트를 집계해봅시다. `event_time` 은 epoch second 값이므로, 일 (Day) 로 변경해야 합니다. [selectExpr](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html) 함수를 쓰면, SQL 식으로 컬럼을 변경할 수 있습니다.

{% hint style="info" %}
시간 값으로 Epoch Second, Epoch Millis 를 사용할 때의 장점은 무엇일까요?&#x20;

* 만약 원본 데이터가 KST 로 되어있을때, 해외에 거주하는 엔지니어 입장에서 데이터를 보면서 디버깅이 쉬울지도 생각해봅시다.
* 만약 다국적 서비스를 하면서 데이터를 한 곳으로 모아야 한다면 어떤 Time Zone 을 쓰는게 맞을까요?
{% endhint %}



`event_time` 을 DATE 로 변경한 후 Group By 해서 일별 구매 이벤트 숫자를 집계해보면 다음과 같습니다. 이 과정에서 구매 이벤트만 필터링 하기 위해 [`where`](2.1.x-spark-dataframe.md#basic-and-aggregation) 함수를 사용합니다.

```python
eventPurchase = "purchase"

df\
    .selectExpr("CAST(event_time as DATE) as event_date", "event_type")\
    .where(col("event_type") == lit(eventPurchase))\
    .groupBy("event_date")\
    .agg(count("*").alias("purchase_count"))\
    .orderBy(asc("event_date"))\
    .show(truncate=False)
```

```
+----------+--------------+
|event_date|purchase_count|
+----------+--------------+
|2020-01-01|3269          |
|2020-01-02|4875          |
|2020-01-03|6031          |
|2020-01-04|6602          |
|2020-01-05|7227          |
|2020-01-06|6368          |
|2020-01-07|7564          |
|2020-01-08|7602          |
|2020-01-09|8774          |
|2020-01-10|8575          |
|2020-01-11|7470          |
|2020-01-12|8147          |
|2020-01-13|10138         |
|2020-01-14|10316         |
|2020-01-15|9962          |
|2020-01-16|9354          |
|2020-01-17|9265          |
|2020-01-18|6197          |
|2020-01-19|8168          |
|2020-01-20|9014          |
+----------+--------------+
```



이제 `where` 함수를 익혔으니 브랜드별 전체 판매 금액에 대해서도 추출해 볼 수 있습니다. 전체 기간 내 판매가 가장 많은 브랜드 10개만 추출해 보겠습니다.

```python
df\
    .selectExpr("brand", "price")\
    .where("brand IS NOT NULL")\
    .groupBy("brand")\
    .agg(sum("price").alias("sales_price"))\
    .orderBy(desc("sales_price"))\
    .limit(10)\
    .show(truncate=False)
```

```
+--------+------------------+
|brand   |sales_price       |
+--------+------------------+
|strong  |2651513.6799999983|
|jessnail|2297451.200000003 |
|runail  |2108654.9900000114|
|irisk   |1467889.08999999  |
|grattol |1055984.4699999897|
|marathon|789238.6300000004 |
|masura  |693152.7399999842 |
|cnd     |599136.3200000003 |
|uno     |546019.0499999991 |
|estel   |544366.7999999983 |
+--------+------------------+
```



{% hint style="info" %}
광고 비즈니스에서는 다음의 용어가 존재합니다.

* ARPU (Average Revenue Per User): 전체 사용자 대비 수익입니다.
* ARPPU (Average Revenue Per Paid User): 결제 사용자 대비 수익입니다.



브랜드별 ARPPU 와 ARPU 를 구해봅시다.
{% endhint %}



```python
df\
    .selectExpr("brand", "price", "user_id", "event_type")\
    .where("brand IS NOT NULL")\
    .groupBy("brand")\
    .agg(
        countDistinct("user_id").alias("user_count_all"), 
        countDistinct(when(col("event_type") == lit(eventPurchase), col("user_id"))).alias("user_count_purchase"), 
        sum("price").alias("sales_price"))\
    .selectExpr(
        "brand", 
        "sales_price", 
        "user_count_all", 
        "sales_price / user_count_all as ARPU",
        "user_count_purchase",
        "sales_price / user_count_purchase as ARPPU",
    )\
    .orderBy(desc("sales_price"))\
    .limit(10)\
    .show(truncate=False)
```

```
+--------+------------------+--------------+------------------+-------------------+------------------+
|brand   |sales_price       |user_count_all|ARPU              |user_count_purchase|ARPPU             |
+--------+------------------+--------------+------------------+-------------------+------------------+
|strong  |2651513.6799999983|5683          |466.5693612528591 |186                |14255.449892473109|
|jessnail|2297451.200000003 |21039         |109.19963876610119|1119               |2053.1288650580905|
|runail  |2108654.9900000114|59595         |35.38308566154898 |9096               |231.8222284520681 |
|irisk   |1467889.08999999  |48028         |30.56319417839573 |7277               |201.71624158306858|
|grattol |1055984.4699999897|34884         |30.271312636165284|4732               |223.15817202028524|
|marathon|789238.6300000004 |2871          |274.9002542668061 |38                 |20769.437631578956|
|masura  |693152.7399999842 |22668         |30.578469207692965|3085               |224.6848427876772 |
|cnd     |599136.3200000003 |9129          |65.63000547705118 |597                |1003.578425460637 |
|uno     |546019.0499999991 |16370         |33.35485949908364 |2446               |223.2293744889612 |
|estel   |544366.7999999983 |24438         |22.27542352074631 |2130               |255.571267605633  |
+--------+------------------+--------------+------------------+-------------------+------------------+
```



### Window Function

이제 몇 가지 조금 더 복잡한 집계를 위해 [Window 함수](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)를 익혀봅니다. Window 함수를 통해 사용자는 특정 집합에 대해 행간의 관계를 집계할 수 있습니다.

* [DataBricks Window Function SQL](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)
* [Expedia: Deep dive into Apache Spark Window Functions](https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-window-functions-7b4e39ad3c86)

\
대부분의 데이터 처리 프레임워크는 Window Function 을 지원합니다.

* [Spark SQL Window Function](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)
* [Presto Window Function](https://prestodb.io/docs/current/functions/window.html)
* [MySQL Window Function](https://dev.mysql.com/doc/refman/8.0/en/window-functions.html) (8.0+)

\
다음 네 가지 문제를 풀어봅시다.

1. 전체 기간동안 브랜드별로 두번째로 많이 팔린 (판매 금액 총합이 높은) 상품 카테고리는 무엇입니까?
2. 일별로 많이 팔린 (판매 금액 총합이 높은) 브랜드별 랭킹 Top 3 는 무엇입니까?
3. 전체 기간동안 브랜드별 판매 금액의 합이나 순위가 아니라, 실제 격차를 나타낼 수 있는 비중을 ([Percentile](https://en.wikipedia.org/wiki/Percentile)) 구하면 어떻게 됩니까?
4. 일별로 모든 브랜드를 통틀어, 판매 금액의 합산을 누적으로 구하면 매출의 변화량은 어떻게 변화합니까?

\
문제를 잘 살펴보면, 단순 Group By 보다는 조금 더 까다로움을 알 수 있습니다. 이해를 돕기 위해 우선 Spark SQL 을 이용해 문제를 풀어보고, 각각의 경우에 대해 PySpark DataFrame 코드도 같이 적어보겠습니다.

\
Spark DataFrame.[createOrReplceTempView](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html) 를 이용하면 DataFrame 을 일종의 Table (더 엄밀히는 실존하지 않고, 테이블처럼만 보이는 View) 와 같이 쓸 수 있습니다. 등록 이후에 [SparkSession.sql ](https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically)을 이용해 만들어진 View (DataFrame) 를 가공할 수 있으며, 이 것의 결과 또한 DataFrame 입니다. 즉 SQL 문법을 이용해 Transformation 을 수행할 수 있습니다.



```python
df.createOrReplaceTempView("PURCHASE")

spark.sql("""
    SELECT *
    FROM PURCHASE
    LIMIT 10
""").show()
```

```
+--------------------+----------------+----------+-------------------+-------------+--------+------+---------+--------------------+
|          event_time|      event_type|product_id|        category_id|category_code|   brand| price|  user_id|        user_session|
+--------------------+----------------+----------+-------------------+-------------+--------+------+---------+--------------------+
|2020-01-01 00:00:...|            view|   5809910|1602943681873052386|         null| grattol|  5.24|595414620|4adb70bb-edbd-498...|
|2020-01-01 00:00:...|            view|   5812943|1487580012121948301|         null|kinetics|  3.97|595414640|c8c5205d-be43-4f1...|
|2020-01-01 00:00:...|            view|   5798924|1783999068867920626|         null|  zinger|  3.97|595412617|46a5010f-bd69-4fb...|
|2020-01-01 00:00:...|            view|   5793052|1487580005754995573|         null|    null|  4.92|420652863|546f6af3-a517-475...|
|2020-01-01 00:00:...|            view|   5899926|2115334439910245200|         null|    null|  3.92|484071203|cff70ddf-529e-4b0...|
|2020-01-01 00:00:...|            view|   5837111|1783999068867920626|         null| staleks|  6.35|595412617|46a5010f-bd69-4fb...|
|2020-01-01 00:00:...|            cart|   5850281|1487580006300255120|         null|marathon|137.78|593016733|848f607c-1d14-474...|
|2020-01-01 00:00:...|            view|   5802440|2151191070908613477|         null|    null|  2.16|595411904|74ca1cd5-5381-4ff...|
|2020-01-01 00:00:...|            view|   5726464|1487580005268456287|         null|    null|  5.56|420652863|546f6af3-a517-475...|
|2020-01-01 00:01:...|remove_from_cart|   5850281|1487580006300255120|         null|marathon|137.78|593016733|848f607c-1d14-474...|
+--------------------+----------------+----------+-------------------+-------------+--------+------+---------+--------------------+
```



이제 SQL / DataFrame 버전으로 각각 Window 함수를 이용해 위 문제를 풀어보겠습니다.

* [Spark SQL Window Function](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)
* [PySpark Window API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.html)



#### 1. 전체 기간동안 브랜드별로 가장 많이 팔린 (판매 금액 총합이 높은) 상품 카테고리

```sql
spark.sql("""
WITH CALCULATED (
    SELECT 
        brand, 
        category_code, 
        sum(price) as price_sum
        
    FROM PURCHASE
    
    WHERE 
        brand IS NOT NULL 
        AND category_code IS NOT NULL
        
    GROUP BY brand, category_code
), 

RANKED (
    SELECT 
        brand, 
        category_code, 
        price_sum, 
        rank() OVER (PARTITION BY brand ORDER BY price_sum DESC) as rank
        
    FROM CALCULATED
)

SELECT *
FROM RANKED
WHERE rank = 1
ORDER BY price_sum DESC
""").show(truncate=False)
```



다음은 PySpark DataFrame 으로 위 코드를 변환한 결과입니다.&#x20;

```python
dfCalculated = df\
    .select(
        col("brand"), 
        col("category_code"), 
        col("price"), 
    )\
    .where(col("brand").isNotNull() & col("category_code").isNotNull())\
    .groupBy("brand", "category_code")\
    .agg(sum("price").alias("price_sum"))

dfRanked = dfCalculated\
    .select(
        col("brand"), 
        col("category_code"),
        col("price_sum"),
        rank().over(Window.partitionBy(col("brand")).orderBy(desc("price_sum"))).alias("rank")
    )

dfRanked\
    .where(col("rank") == lit(1))\
    .orderBy(desc("price_sum"))\
    .show(truncate=False)
```

결과는 아래와 같이 동일합니다.

```
+---------+-------------------------------+------------------+----+
|brand    |category_code                  |price_sum         |rank|
+---------+-------------------------------+------------------+----+
|max      |appliances.environment.vacuum  |489141.04999999976|1   |
|polarus  |appliances.environment.vacuum  |418171.82000000007|1   |
|emil     |appliances.environment.vacuum  |296071.9100000001 |1   |
|jessnail |appliances.environment.vacuum  |136883.68         |1   |
|runail   |furniture.living_room.cabinet  |125213.18000000004|1   |
|irisk    |furniture.bathroom.bath        |79211.96          |1   |
|vosev    |accessories.bag                |48592.50000000001 |1   |
|benovy   |apparel.glove                  |43165.819999999985|1   |
|kosmekka |furniture.living_room.cabinet  |42673.57000000006 |1   |
|italwax  |stationery.cartrige            |17865.97999999999 |1   |
|jaguar   |appliances.personal.hair_cutter|16316.279999999988|1   |
|nitrimax |apparel.glove                  |14064.090000000006|1   |
|nitrile  |apparel.glove                  |10399.849999999999|1   |
|kondor   |appliances.personal.hair_cutter|9074.939999999997 |1   |
|domix    |furniture.bathroom.bath        |5248.419999999998 |1   |
|shik     |accessories.cosmetic_bag       |4122.72           |1   |
|depilflax|stationery.cartrige            |3861.7799999999966|1   |
|gezatone |appliances.personal.massager   |3240.320000000001 |1   |
|naturmed |furniture.bathroom.bath        |2997.71           |1   |
|naomi    |apparel.glove                  |2002.319999999998 |1   |
+---------+-------------------------------+------------------+----+
```



#### 2. 일별로 많이 팔린 (판매 금액 총합이 높은) 브랜드별 랭킹 Top 3

```sql
spark.sql("""
WITH CALCULATED (
    SELECT 
        CAST(event_time AS DATE) as event_date,
        brand, 
        sum(price) as price_sum
        
    FROM PURCHASE
    
    WHERE 
        brand IS NOT NULL 
        
    GROUP BY 1, 2
), 

RANKED (
    SELECT 
        event_date,
        brand, 
        price_sum, 
        rank() OVER (PARTITION BY event_date ORDER BY price_sum DESC) as rank
        
    FROM CALCULATED
)

SELECT *
FROM RANKED
WHERE rank <= 3
ORDER BY event_date ASC, rank ASC
""").show(truncate=False)
```

```
+----------+--------+------------------+----+
|event_date|brand   |price_sum         |rank|
+----------+--------+------------------+----+
|2020-01-01|jessnail|58125.96000000012 |1   |
|2020-01-01|strong  |45510.60999999997 |2   |
|2020-01-01|runail  |40028.73999999981 |3   |
|2020-01-02|jessnail|75567.9100000001  |1   |
|2020-01-02|strong  |53850.130000000005|2   |
|2020-01-02|runail  |46773.700000000055|3   |
|2020-01-03|jessnail|130618.19000000042|1   |
|2020-01-03|strong  |67431.65          |2   |
|2020-01-03|runail  |65798.47999999997 |3   |
|2020-01-04|jessnail|77021.25000000009 |1   |
|2020-01-04|strong  |72237.20999999993 |2   |
|2020-01-04|runail  |55250.03000000005 |3   |
|2020-01-05|jessnail|83268.23000000014 |1   |
|2020-01-05|strong  |69062.75999999997 |2   |
|2020-01-05|runail  |56429.79000000005 |3   |
|2020-01-06|strong  |80781.73999999989 |1   |
|2020-01-06|jessnail|66437.16000000012 |2   |
|2020-01-06|runail  |50932.03000000017 |3   |
|2020-01-07|jessnail|76882.80000000008 |1   |
|2020-01-07|strong  |74643.85999999997 |2   |
+----------+--------+------------------+----+
```



{% hint style="info" %}
일별로 가장 많이 팔린 브랜드를 정렬해 Top 3 개의 브랜드를 추출하는 코드를 DataFrame 으로 만들어 봅시다.
{% endhint %}



#### 3. 전체 기간동안 브랜드별 매출(판매 금액의 합) 을 구하되, 자신보다 한단계 높은 순위 또는 낮은 순위의 매출도 같이 표시하기

```sql
spark.sql("""
WITH CALCULATED (
    SELECT 
        brand, 
        sum(price) as price_sum
        
    FROM PURCHASE
    
    WHERE 
        brand IS NOT NULL 
        
    GROUP BY brand
), 

RANKED (
    SELECT 
        brand, 
        lag(price_sum, 1) OVER (PARTITION BY 1 ORDER BY price_sum DESC) as price_sum_prev,
        price_sum as price_sum_current, 
        lead(price_sum, 1) OVER (PARTITION BY 1 ORDER BY price_sum DESC) as price_sum_next,
        dense_rank() OVER (PARTITION BY 1 ORDER BY price_sum DESC) as rank        
        
    FROM CALCULATED
)

SELECT *
FROM RANKED
ORDER BY rank ASC
""").show(truncate=False)
```

```
|brand    |price_sum_prev    |price_sum_current |price_sum_next    |rank|
+---------+------------------+------------------+------------------+----+
|strong   |null              |2651513.6799999983|2107905.480000001 |1   |
|jessnail |2651513.6799999983|2107905.480000001 |1877075.9400000153|2   |
|runail   |2107905.480000001 |1877075.9400000153|1280128.789999989 |3   |
|irisk    |1877075.9400000153|1280128.789999989 |1055984.4699999897|4   |
|grattol  |1280128.789999989 |1055984.4699999897|789238.6300000004 |5   |
|marathon |1055984.4699999897|789238.6300000004 |693014.0199999842 |6   |
|masura   |789238.6300000004 |693014.0199999842 |599136.3200000003 |7   |
|cnd      |693014.0199999842 |599136.3200000003 |546019.0499999991 |8   |
|uno      |599136.3200000003 |546019.0499999991 |544323.2499999983 |9   |
|estel    |546019.0499999991 |544323.2499999983 |489141.04999999976|10  |
|max      |544323.2499999983 |489141.04999999976|427986.93999999884|11  |
|ingarden |489141.04999999976|427986.93999999884|418171.82000000007|12  |
|polarus  |427986.93999999884|418171.82000000007|365795.62000000005|13  |
|italwax  |418171.82000000007|365795.62000000005|306945.5899999999 |14  |
|browxenna|365795.62000000005|306945.5899999999 |296071.9100000001 |15  |
|emil     |306945.5899999999 |296071.9100000001 |293430.7299999994 |16  |
|kapous   |296071.9100000001 |293430.7299999994 |293421.13         |17  |
|shik     |293430.7299999994 |293421.13         |260097.03000000014|18  |
|jas      |293421.13         |260097.03000000014|254294.83999999994|19  |
|kosmekka |260097.03000000014|254294.83999999994|245135.18000000008|20  |
+---------+------------------+------------------+------------------+----+
```

{% hint style="info" %}
위의 SQL 코드를 DataFrame 으로 변경해봅시다.
{% endhint %}

{% hint style="info" %}
rank() 와 dense\_rank() 의 차이는 무엇일까요? row\_number() 등 다른 함수도 살펴봅시다.

* [Spark Internals - Spark Window Functions](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-functions-windows.html)
{% endhint %}



#### 4. 일별로 모든 브랜드를 통틀어, 판매 금액의 합산을 누적으로 구하면 매출의 변화량은 어떤지 살펴보기

```sql
spark.sql("""
WITH CALCULATED (
    SELECT 
        CAST(event_time AS DATE) as event_date,
        sum(price) as price_sum
        
    FROM PURCHASE
    
    WHERE 
        brand IS NOT NULL 
        
    GROUP BY 1
), 

RANKED (
    SELECT 
        event_date,
        price_sum,
        sum(price_sum) OVER (PARTITION BY 1 ORDER BY event_date RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as price_acc
        
    FROM CALCULATED
)

SELECT *
FROM RANKED
ORDER BY event_date ASC
""").show(truncate=False)
```

```
+----------+------------------+--------------------+
|event_date|price_sum         |price_acc           |
+----------+------------------+--------------------+
|2020-01-01|478880.47999999445|478880.47999999445  |
|2020-01-02|591996.0299999958 |1070876.5099999902  |
|2020-01-03|764825.2799999974 |1835701.7899999875  |
|2020-01-04|672783.4599999917 |2508485.249999979   |
|2020-01-05|698293.9199999882 |3206779.1699999673  |
|2020-01-06|648607.0899999944 |3855386.2599999616  |
|2020-01-07|702788.3999999919 |4558174.659999954   |
|2020-01-08|710164.8099999978 |5268339.469999951   |
|2020-01-09|786942.4299999884 |6055281.89999994    |
|2020-01-10|769210.5999999931 |6824492.499999933   |
|2020-01-11|732801.5299999891 |7557294.029999922   |
|2020-01-12|818940.0799999872 |8376234.109999909   |
|2020-01-13|860498.5800000073 |9236732.689999916   |
|2020-01-14|867423.4899999918 |1.0104156179999907E7|
|2020-01-15|858057.1199999887 |1.0962213299999895E7|
|2020-01-16|788017.7599999914 |1.1750231059999887E7|
|2020-01-17|771007.1899999875 |1.2521238249999875E7|
|2020-01-18|714140.5399999954 |1.323537878999987E7 |
|2020-01-19|781291.109999992  |1.4016669899999863E7|
|2020-01-20|849404.3399999914 |1.4866074239999853E7|
+----------+------------------+--------------------+
only showing top 20 rows
```



{% hint style="info" %}
Window Function 이 돌아갈때의 데이터 흐름을 생각해봅시다.

Shuffle 이 발생할까요? 발생한다면 어떻게 데이터가 나누어지고, 집계될까요?
{% endhint %}



### Window Function - Attribution

![Attribution Model Overview (https://www.silverdisc.co.uk/blog/2017/09/22/attribution-modelling-%E2%80%93-moving-away-last-click-attribution)](<../../.gitbook/assets/image (5).png>)



광고 또는 마케팅에는 기여도 (Attribution) 이라는 개념이 있습니다. 광고 또는 마케팅 켐페인을 위해 지불한 비용의 효과를 측정하기 위해, "원하는 Action 을 이끌어낸 것이 내가 돈을 지불한 광고가 맞느냐" 를 판별하기 위한 방법입니다.&#x20;

다음과 같은 우동마켓의 시나리오를 가정해 봅시다.

* 사용자 A 는 우동마켓 앱 내에서 검색 키워드 K1 (어묵우동) 이후에 상품을 P1, P2, P3 를  탐색합니다.
* 사용자 A 는 검색 키워드 K2 (대파우동) 로 검색을 해 P3, P4, P5 를 탐색합니다.
* 사용자 A 는 어느정도 시간이 지난 후 마음의 결정을 해 상품 P3 를 구매합니다.

이 때, 데이터의 유실이 없고 키워드 검색 이벤트가 상품 클릭 이벤트보다 항상 먼저 들어온다고 가정하면

* `(A, K1, [P1, P2, P3])`
* `(A, K2, [P3, P4, P5])`

\
위 처럼 데이터를 묶어, 아래와 같이 만들어 볼 수 있을 것입니다. 사용자 A 가 P3 를 주문했을때,

* 그 주문을 위해 탐색한 키워드 K1 (Non-last Click간접 기여), K3 (Last Click, 직접 기여)
* `(A, [K1, K3], P3)`

\
위와 같이 이벤트를 Window 함수로 묶어 K1, K3 가 사용자 A 의 상품 주문 P3 에 기여했음을 만들 수 있지 않을까요? Spark Window Function 에서 제공되는 다음의 함수를 통해 어떻게 Attribution 데이터를 가공할지 생각해 봅시다.

* [Spark Docs - Spark Window Function](2.1.x-spark-dataframe.md#basic-and-aggregation)
* [DataBricks - Calculate Last Touch Attribution on View](https://databricks.com/blog/2018/08/09/building-a-real-time-attribution-pipeline-with-databricks-delta.html)

{% hint style="info" %}
만약 배치가 아니라 실시간으로 Attribution 을 계산해야 한다면 어떤 제약이 있을까요?

사용자가 상품을 본 뒤로 최대 몇시간까지 기다려야 할지 논의해 봅시다.
{% endhint %}





### Window Function - Session

데이터 분석에는 일반적으로 **Session** 이라는 개념이 자주 사용됩니다.&#x20;

동일한 사용자라도, 특정 시점에는 특정 상품에 관심이 있다는 이론을 바탕으로 사용자를 지정된 '시간' 단위로 나눌 수 있습니다.&#x20;

예를 들어 Google Analytics (GA) 에서는 시간 기반으로 세션을 나눌때, 활동이 없을 경우 30 분이 지나면 새로운 세션으로 구분됩니다.

![Session Overview (Google Analytisc)](<../../.gitbook/assets/image (6).png>)



위 그림에서 사용자는 14:04 가 마지막 이벤트라고 가정하면, 14:35 에 다시 이벤트가 발생했을 때 다른 세션 ID 를 발급 받습니다.&#x20;

* User A, Session 1 이벤트: 14:04 까지
* User B, Session 2 이벤트: 14:35 부터 발생하는 이벤트

만약 `ecommerce_event` 데이터 셋을 이용해 세션을 만든다면, Window 함수를 어떻게 사용할 수 있을까요? 다음 문서를 참조해 봅시다,

* [https://stackoverflow.com/questions/54662219/spark-advanced-window-with-dynamic-last](https://stackoverflow.com/questions/54662219/spark-advanced-window-with-dynamic-last)



구현에 참조할 만한 몇 가지 아이디어는 다음과 같습니다.

* Window Function 중 Lag 는, 현재 집합 (Window) 에 대해 이전의 값을 현재 컬럼에 붙일 수 있습니다.
* event\_time - Lag(event\_time) > Threshold



{% hint style="info" %}
Spark 는 Session 생성을 위한 Window 함수를 제공하기도 합니다. 직접 만들었다면, 이 함수를 이용해서 구현해봅시다.

* [Spark Docs - Session Window](https://spark.apache.org/docs/latest/api/sql/index.html#session\_window)
{% endhint %}







### Array Type & Unnest





### JSON Function & Struct Type





