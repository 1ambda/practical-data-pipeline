# 2.1.8 Spark SQL & Table

이번 챕터에서는 Spark SQL 로 테이블을 다루기 위한 Hive Metastore / Partition 등의 개념과 API 에 대해 알아봅니다.

사용할 데이터는 [AirBnB Denver](https://www.kaggle.com/broach/denverairbnb) 데이터셋 중 airbnb\_calendar.csv 와 airbnb\_listings.csv 입니다.

* `airbnb_listings.csv` 는 AirBnB 숙소의 메타 정보를 담고 있습니다
* `airbnb_calendar.csv` 는 AirBnB 숙소의 특정 일자별 가격과 투숙일 정보를 담고 있습니다



```python
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql.window import Window

dfCalendar = spark.read.load("./airbnb_calendar.csv",
                     format="csv", inferSchema=True, header=True,
                     quote='"', escape='"', sep=',', multiline=True)


dfListing = spark.read.load("./airbnb_listings.csv",
                     format="csv", inferSchema=True, header=True,
                     quote='"', escape='"', sep=',', multiline=True)
                     
dfCalendar = dfCalendar.cache()
dfListing = dfListing.cache()

dfCalendar.printSchema()
dfCalendar.show()
dfListing.printSchema()
dfListing\
   .select("id", "listing_url", "name", "description", "property_type", "city", "review_scores_rating", "price")\
   .show()
```



**dfCalendar** 와 **dfListing** 의 스키마와 샘플 데이터를 살펴보겠습니다.

```
# dfCalendar.printSchema()

root
 |-- listing_id: integer (nullable = true)
 |-- date: string (nullable = true)
 |-- available: string (nullable = true)
 |-- price: string (nullable = true)
 |-- adjusted_price: string (nullable = true)
 |-- minimum_nights: integer (nullable = true)
 |-- maximum_nights: integer (nullable = true)

dfCalendar.show()
+----------+----------+---------+-------+--------------+--------------+--------------+
|listing_id|      date|available|  price|adjusted_price|minimum_nights|maximum_nights|
+----------+----------+---------+-------+--------------+--------------+--------------+
|   1153002|2019-11-29|        f|$145.00|       $145.00|            30|            60|
|   3138055|2019-11-29|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-11-30|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-01|        t|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-02|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-03|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-04|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-05|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-06|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-07|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-08|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-09|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-10|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-11|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-12|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-13|        t|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-14|        t|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-15|        t|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-16|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-17|        f|$130.00|       $130.00|             2|          1125|
+----------+----------+---------+-------+--------------+--------------+--------------+
```

```
# dfListing.printSchema()
root
 |-- id: integer (nullable = true)
 |-- listing_url: string (nullable = true)
 |-- scrape_id: long (nullable = true)
 |-- last_scraped: string (nullable = true)
 |-- name: string (nullable = true)
 |-- summary: string (nullable = true)
 |-- space: string (nullable = true)
 |-- description: string (nullable = true)
 |-- experiences_offered: string (nullable = true)
 |-- neighborhood_overview: string (nullable = true)
 |-- notes: string (nullable = true)
 |-- transit: string (nullable = true)
 |-- access: string (nullable = true)
 |-- interaction: string (nullable = true)
 |-- house_rules: string (nullable = true)
 |-- thumbnail_url: string (nullable = true)
 |-- medium_url: string (nullable = true)
 |-- picture_url: string (nullable = true)
 |-- xl_picture_url: string (nullable = true)
 |-- host_id: integer (nullable = true)
 |-- host_url: string (nullable = true)
 |-- host_name: string (nullable = true)
...

# dfListing\
#    .select("id", "listing_url", "name", "description", "property_type", "city", "review_scores_rating", "price")\
#    .show()
+------+--------------------+--------------------+--------------------+-------------+---------+--------------------+-------+
|    id|         listing_url|                name|         description|property_type|     city|review_scores_rating|  price|
+------+--------------------+--------------------+--------------------+-------------+---------+--------------------+-------+
|   360|https://www.airbn...|LoHi Secret garde...|Come enjoy our oa...|   Guesthouse|   Denver|                 100|$140.00|
|   590|https://www.airbn...|Comfortable  - an...|Large guest room ...|        House|   Denver|                  97| $61.00|
|   592|https://www.airbn...|             private|This room is in t...|        House|   Denver|                  97| $44.00|
|  1940|https://www.airbn...|Baker Studio Clos...|Great place for a...|   Guesthouse|   Denver|                  99| $95.00|
|  2086|https://www.airbn...|  Garden Level Condo|A furnished, gard...|    Apartment|   Denver|                  96| $76.00|
| 31503|https://www.airbn...|Highland Park Gue...|Highland Park Gue...|  Guest suite|   Denver|                  98|$110.00|
| 39405|https://www.airbn...|LoHi Secret garde...|Come enjoy our oa...|      Cottage|   Denver|                  98|$111.00|
| 56185|https://www.airbn...|charming home for...|Spend time in Den...|        House|   Denver|                  98|$300.00|
| 59631|https://www.airbn...|VICTORIAN TOWNHOM...|License #2017-BFN...|    Townhouse|   Denver|                  96|$179.00|
| 74125|https://www.airbn...|Spacious Cap Hill...|1000' entire-firs...|    Apartment|   Denver|                  94|$130.00|
| 81540|https://www.airbn...|Affordable S. Den...|Bright, sunny 1 b...|   Guesthouse|Englewood|                  98| $75.00|
| 90307|https://www.airbn...|Comfy King Size R...|This private bedr...|        House|   Denver|                  98| $90.00|
| 98008|https://www.airbn...|Beautiful sun fil...|Locaton, location...|        House|   Denver|                  99| $70.00|
| 98014|https://www.airbn...|Beautiful single ...|Hi Folks!  Welcom...|        House|   Denver|                  98| $60.00|
|142683|https://www.airbn...|Historic Denver C...|One of three cond...|        House|   Denver|                  94|$125.00|
|172196|https://www.airbn...|Luxury Wash Park ...|Remodeled wash pa...|        House|   Denver|                  99|$411.00|
|184529|https://www.airbn...|HIP SUITE IN  WES...|Private SUIITE   ...|  Guest suite|   Denver|                  97| $35.00|
|192430|https://www.airbn...|TREETOP VIEW ROOM...|Located in the de...|  Condominium|   Denver|                  94| $32.00|
|217996|https://www.airbn...|       Highland Snug|Comfortable and c...|  Condominium|   Denver|                 100| $66.00|
|236207|https://www.airbn...|Denver Penthouse ...|Important Note : ...|         Loft|   Denver|                  99|$235.00|
+------+--------------------+--------------------+--------------------+-------------+---------+--------------------+-------+
```



### Spark SQL Table API

지난 챕터에서 언급한바와 같이 Spark DataFrame API 는 Spark SQL 을 통해서도 사용 가능합니다.

* [createOrReplaceTempView](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html) 를 통해 DataFrame 을 View 로 등록해 SQL 구문에서 사용할 수 있고 (spark.catalog.DropView 를 통해 제거)
* 일반적인 연산을 위한 [SQL Function](https://spark.apache.org/docs/latest/api/sql/index.html) 이나 [Window Function](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html) 은 물론 [Array, Map, JSON](https://spark.apache.org/docs/latest/sql-ref-functions.html) 과 같은 다양한 타입과 그를 위한 함수들도 제공합니다.
* 필요하다면 사용자 정의 함수 ([User Defined Function, UDF](https://spark.apache.org/docs/latest/sql-ref-functions.html)) 를 등록해 SQL 구문 내에서 사용하는 것이 가능하며
* DataFrame.cache() 와 같은 인프라 수준의 기능도 [CACHE TABLE](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html) 구문을 통해 지원합니다.



Spark SQL 을 실습해보며 기본적인 사용법을 익혀보겠습니다. \
DataFrame.[createOrReplaceTempView()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html#pyspark.sql.DataFrame.createOrReplaceTempView) 함수를 통해 DataFrame 을 View 로 등록해 SQL 내에서 테이블처럼 사용할 수 있습니다.

* 더 엄밀히는, DataFrame 에 대한 in-memory Reference 를 만듭니다
* 즉, 테이블을 생성하는 것이 아니라 View (테이블처럼 보이지만 실제로는 연산이 필요한) 를 만듭니다



```python
dfListing.createOrReplaceTempView("LISTING_META")
dfCalendar.createOrReplaceTempView("LISTING_CALENDAR")

spark.sql("""
SELECT id, listing_url, name, property_type
FROM LISTING_META
LIMIT 10
""").show()

spark.sql("""
SELECT *
FROM LISTING_CALENDAR
LIMIT 10
""").show()
```

```
+-----+--------------------+--------------------+-------------+
|   id|         listing_url|                name|property_type|
+-----+--------------------+--------------------+-------------+
|  360|https://www.airbn...|LoHi Secret garde...|   Guesthouse|
|  590|https://www.airbn...|Comfortable  - an...|        House|
|  592|https://www.airbn...|             private|        House|
| 1940|https://www.airbn...|Baker Studio Clos...|   Guesthouse|
| 2086|https://www.airbn...|  Garden Level Condo|    Apartment|
|31503|https://www.airbn...|Highland Park Gue...|  Guest suite|
|39405|https://www.airbn...|LoHi Secret garde...|      Cottage|
|56185|https://www.airbn...|charming home for...|        House|
|59631|https://www.airbn...|VICTORIAN TOWNHOM...|    Townhouse|
|74125|https://www.airbn...|Spacious Cap Hill...|    Apartment|
+-----+--------------------+--------------------+-------------+

+----------+----------+---------+-------+--------------+--------------+--------------+
|listing_id|      date|available|  price|adjusted_price|minimum_nights|maximum_nights|
+----------+----------+---------+-------+--------------+--------------+--------------+
|   1153002|2019-11-29|        f|$145.00|       $145.00|            30|            60|
|   3138055|2019-11-29|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-11-30|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-01|        t|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-02|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-03|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-04|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-05|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-06|        f|$130.00|       $130.00|             2|          1125|
|   3138055|2019-12-07|        f|$130.00|       $130.00|             2|          1125|
+----------+----------+---------+-------+--------------+--------------+--------------+
```

![Spark DataFame API - createGlobalTempView](<../../.gitbook/assets/image (5).png>)

Jupyter 에서 자동완성을 통해 DataFrame 이 지원하는 함수를 보면 '**GlobalTempView**' 라는것이 보입니다.&#x20;

* **createTempView**, **createOrReplaceTempView** 를 통해 만든 View 는 해당 Spark Session 내에서 사용이 가능합니다
* 하지만 Global 로 만들면 서로 다른 Spark Session 간 공유가 가능합니다.



```python
propertyTypeHouse = 'House'

dfListingHouseAvailable = spark.sql(f"""
WITH 

LISTING_HOUSE AS (
    SELECT id as listing_id, listing_url, name
    FROM LISTING_META
    WHERE property_type = '{propertyTypeHouse}'
)

SELECT 
    LISTING_CALENDAR.listing_id, 
    listing_url, 
    name as listing_name,
    date,
    available,
    price
    
FROM LISTING_CALENDAR
INNER JOIN LISTING_HOUSE
    ON LISTING_CALENDAR.listing_id = LISTING_HOUSE.listing_id
WHERE LISTING_CALENDAR.available = 't'
""")
```

```
# dfListingHouseAvailable.show()

+----------+--------------------+--------------------+----------+---------+-------+
|listing_id|         listing_url|        listing_name|      date|available|  price|
+----------+--------------------+--------------------+----------+---------+-------+
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-01|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-13|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-14|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-15|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-21|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-22|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-26|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-27|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-28|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2019-12-29|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-03|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-04|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-05|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-10|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-11|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-12|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-17|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-18|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-19|        t|$130.00|
|   3138055|https://www.airbn...|Classic Home on C...|2020-01-24|        t|$130.00|
+----------+--------------------+--------------------+----------+---------+-------+
```



위에서 사용한 쿼리 예제에서는 **LISTING\_META** (Product Meta) 와 **LISTING\_CALENDAR**_ _(Product Meta) 테이블을 조인했습니다.



Product Meta + Product Event  (Transaction, Calendar, Activity) 조합은 앞으로 여러분들이 굉장히 많이 사용할 데이터 결합 패턴입니다.

* 상품의 속성은 일반적으로 별도 테이블로 존재하고 상품 이벤트에 비해 비교적 적은 Row 로 구성되어 있습니다
* 상품의 이벤트는 일반적으로 상품 메타에 비해 데이터 사이즈가 매우 큽니다
* 예를 들어, 상품은 1개여도 해당 상품에 대한 주문은 여러번 발생할 수 있습니다
  * 도메인에 따라 상품 메타가 더 자세히 나뉘기도 합니다
  * 커머스의 경우에는 상품 > 상품 옵션일수도 있습니다
  * 여행의 경우에는 호텔 > 룸 > 레이트 플랜 (룸 + 옵션) > 날짜 와 같이 더 복잡한 형태로 구성되기도 합니다



위와 같이 상품 이벤트에 대해 상품 메타를 붙이는 경우도 있지만 많은 경우에 상품 메타를 축으로 잡고 집계 연산을 수행할 수 있습니다. (e.g, 통계 데이터 등)

```python
dfListingAvailability = spark.sql("""
WITH LISTING_GROUPED AS (
    SELECT 
        listing_id, 
        count(CASE WHEN available = 't' THEN 1 END) as count_date_available,
        count(CASE WHEN available <> 't' THEN 1 END) as count_date_unavailable
        
    FROM LISTING_CALENDAR
    GROUP BY listing_id
)

SELECT 
    LISTING_META.id as listing_id,
    LISTING_META.listing_url as listing_url,
    LISTING_META.name as listing_name,
    LISTING_META.review_scores_rating as review_scores_rating,
    LISTING_META.price as listing_price_basic,
    coalesce(LISTING_GROUPED.count_date_available, 0) as count_date_available,
    coalesce(LISTING_GROUPED.count_date_unavailable, 0) as count_date_unavailable

FROM LISTING_META
LEFT JOIN LISTING_GROUPED 
    ON LISTING_META.id = LISTING_GROUPED.listing_id
""")
```

```
# dfListingAvailability.orderBy(desc("count_date_unavailable")).show()

+----------+--------------------+--------------------+--------------------+-------------------+--------------------+----------------------+
|listing_id|         listing_url|        listing_name|review_scores_rating|listing_price_basic|count_date_available|count_date_unavailable|
+----------+--------------------+--------------------+--------------------+-------------------+--------------------+----------------------+
|  19699920|https://www.airbn...|One bedroom in an...|                  97|             $80.00|                   0|                   366|
|  23418604|https://www.airbn...|Bright & Cozy Apt...|                 100|            $115.00|                   0|                   366|
|  24306031|https://www.airbn...|Historic Duplex L...|                null|             $88.00|                   0|                   366|
|  37912396|https://www.airbn...|Happy Travelers W...|                 100|             $75.00|                   0|                   366|
|  19154631|https://www.airbn...|Newly Remodeled H...|                 100|            $175.00|                   0|                   366|
|  11533087|https://www.airbn...|Beautiful Home by...|                  96|            $225.00|                   0|                   366|
|  18503556|https://www.airbn...|Spacious Studio i...|                 100|            $119.00|                   0|                   366|
|  20425070|https://www.airbn...|Location, locatio...|                  91|            $150.00|                   0|                   366|
|  39309393|https://www.airbn...|Guest suite Denve...|                null|             $80.00|                   0|                   366|
|  14154970|https://www.airbn...|Best Location! Gr...|                  99|            $229.00|                   0|                   366|
|  27332948|https://www.airbn...|Cozy Modern Bedro...|                 100|             $48.00|                   0|                   366|
|  12172692|https://www.airbn...|Open, bright, mou...|                  93|            $145.00|                   0|                   366|
|  32017055|https://www.airbn...|⭐️ Private & Cent...|                  99|             $49.00|                   0|                   366|
|  27422441|https://www.airbn...|Friendliest Place...|                 100|            $101.00|                   0|                   365|
|  27759634|https://www.airbn...|Cozy Room in Down...|                 100|             $55.00|                   0|                   365|
|  19752351|https://www.airbn...|Beautiful townhom...|                null|            $150.00|                   0|                   365|
|  27259874|https://www.airbn...|Shared Denver Hou...|                  87|             $77.00|                   0|                   365|
|  36149907|https://www.airbn...|        The Nest 2.0|                 100|             $90.00|                   0|                   365|
|  11509014|https://www.airbn...|Garden Suite - Bo...|                 100|             $80.00|                   0|                   365|
|  21781844|https://www.airbn...|         Beats Motel|                null|             $30.00|                   0|                   365|
+----------+--------------------+--------------------+--------------------+-------------------+--------------------+----------------------+
```



상품 메타 (**LISTING\_META**) 가 기준이 되는 테이블이고, 상품 이벤트 (**LISTING\_CALENDAR**) 를 요약해서 [LEFT JOIN](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html) 을 수행했습니다.

* 다만 이 때, 상품 이벤트가 존재하지 않을 수 있으므로 (주문이 없거나 누락 등) [coalesce](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.coalesce.html) 함수를 이용해서 NULL 값이 나오지 않도록 합니다
* 위의 예제와 같이 만약 0 값이 비즈니스적으로 어떤 의미를 가진다면  [coalesce](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.coalesce.html) 를 사용하지 않고 0 대신 NULL 을 사용할수도 있습니다
* **LISTING\_CALENDAR** 는 _**listing\_id**_ 당 중복되는 'date' 가 없다고 가정합니다. 만약 중복이 있다면 distinct 를 사용하거나 중복이 없는 상품 메타 단위로 더 세분화 해 집계할 수 있습니다

통계 등을 위한 조인 및 집계 관련해서는 추후에 다른 챕터에서 더 설명하도록 하겠습니다. 아래 그림을 통해 JOIN 종류에 따른 결과를 시각적으로 볼 수 있습니다.

![SQL Join (https://clickatech.co.za/back-to-basics-sql-joins/)](<../../.gitbook/assets/image (12).png>)





이제 **LISTING\_AVAILABILITY** 를 View 로 등록하고 `review_scores_rating > 0` 인 경우에만 **LISTING\_STAT** View 로 만든 뒤 [CACHE TABLE](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html) 구문을 이용해 캐싱해보겠습니다.

```python
spark.sql("""
CACHE TABLE LISTING_STAT AS (
    SELECT *
    FROM LISTING_AVAILABILITY
    WHERE review_scores_rating IS NOT NULL
)
""")

spark.sql("SHOW TABLES").show()
```

```
+--------+--------------------+-----------+
|database|           tableName|isTemporary|
+--------+--------------------+-----------+
|        |listing_availability|       true|
|        |    listing_calendar|       true|
|        |        listing_meta|       true|
|        |        listing_stat|       true|
+--------+--------------------+-----------+
```



위의 예제에서는 spark.sql 을 이용해 현재 생성된 View (임시 테이블) 을 목록을 확인했습니다. [Spark Catalog API](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Catalog.html) 를 이용해서도 가능합니다. Catalog API 는 테이블 생성 / 조회 등 Metastore 관련된 함수들을 제공합니다.

* [Catalog API 전체 목록](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Catalog.html?highlight=catalog)

```
# spark.catalog.listTables().show()

[Table(name='listing_availability', database=None, description=None, tableType='TEMPORARY', isTemporary=True),
 Table(name='listing_calendar', database=None, description=None, tableType='TEMPORARY', isTemporary=True),
 Table(name='listing_meta', database=None, description=None, tableType='TEMPORARY', isTemporary=True),
 Table(name='listing_stat', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]
```



이제까지의 예제에서는 DataFrame.createOrReplaceTempVeiw() 를 사용해서 View 를 만들었지만 [CREATE VIEW](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-view.html) 구문을 이용하면 위의 [CACHE TABLE AS (...)](https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html) 과 같이 SQL 쿼리를 이용해서 View 를 생성하는것도 가능합니다.

* [Spark Docs - Create View](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-view.html)
* [Spark Docs - SQL Syntax](https://spark.apache.org/docs/latest/sql-ref-syntax.html)



{% hint style="info" %}
Spark SQL 과 Spark DataFrame API 는 많은 경우에 혼용이 가능합니다. 그렇다면 언제 어떤 API 를 사용하는 것이 맞을까요? 다음의 경우를 생각해 봅시다.

* SQL 에 익숙한 분석가가 Spark 를 사용하는 경우
* 클래스와 함수를 사용하는데 익숙한 엔지니어가 Spark 를 사용하는 경우

\
그리고, 다루는 사람의 역할이 아니라 생산성과 사용성 관점에서 Spark SQL 과 DataFrame API 가 가지는 장점에 대해서도 이야기 해봅시다.

* 만약 둘 다 섞어서 사용한다면, 어느 부분에 어떤 API 를 사용하는것이 이점이 있을까요?
{% endhint %}









### Spark w/ Hive Metastore



### Partitioned Table



### Spark and Hive
