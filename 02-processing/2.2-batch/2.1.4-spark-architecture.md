# 2.1.4 Spark Architecture

### Driver

\
Spark 는 크게 두 가지 컴포넌트로 구성되어 있습니다. **Driver** 와 **Executor** 입니다.

* Driver 는 1개로, 사용자의 Main 함수, 즉 사용자가 작성한 로직을 실행합니다. 이 과정에서 실행 계획을 생성해 Executor 에게 Task 를 할당할 수 있습니다.
* Driver 는 Cluster Manager 와 통신하며 Spark Application 관리합니다.
* Executor 는 여러개가 될 수 있습니다. 분산처리를 위해 Driver 가 요청한 계산 (Task) 을 수행할 수 있습니다. 이 결과는 Action 에 따라 (collect, foreachPartition) Driver 로 다시 돌려 보낼 수 있습니다.
* Executor 는 사용자가 cache() 와 같은 함수 호출시 데이터를 메모리 (또는 디스크) 에 저장할 수 있습니다.

\
Cluster Manager 는 본 챕터의 아래 섹션에서 설명하겠지만, 다수개의 Spark 작업을 실행할 수 있도록 리소스를 관리해주는 Hadoop / AWS EMR 의 Yarn 혹은 Kubernetes 와 같은 클러스터를 말합니다.\


![Spark Cluster Mode Overview (https://spark.apache.org/docs/latest/cluster-overview.html)](<../../.gitbook/assets/image (2).png>)

\
\
Spring Boot 와 같은 단일 머신을 위한 API Framework 와 다르게 Spark 의 코드는 Driver 와 Executor 에서 나누어 돌아갑니다. 지난 챕터에서 다루지 않았던 Action 몇 가지를 살펴보며 이 부분을 알아보겠습니다.\


지난 챕터 Spark Concept 에서 다루었듯이, 데이터는 Partition 으로 구분되며 Executor 가 나누어 처리할 수 있다고 이야기 했었는데 Spark Dataframe 의 [collect()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.collect.html) Action 을 사용하면 Executor 에 분산되어 있던 데이터를 Driver 로 모을 수 있습니다.



```
collected = dfPartitioned.collect()

# type(collected) 의 실행 결과
list

# collected[0] 의 실행 결과
Row(id=7196, year_birth=1950, education='PhD', count_kid=1, count_teen=1, date_customer='08-02-2014', days_last_login=20, date_joined=datetime.date(2020, 2, 8))
```

\
collect() 를 통해 가져온 데이터는 더이상 Spark 의 `DataFrame` 이 아니기에 분산되어 있는 데이터가 아닙니다. 타입도 일반 `list` 이며 Driver 메모리에 존재하는 일반 변수이므로 다른 변수와 같이 연산할 수 있습니다.

```
from pyspark.sql import Row

missing_days = 10

# Spark 의 Row 는 read-only 입니다. 따라서 Python 에서 변경하기 위해 Dict 로 변경 후 다시 Row 로 되돌립니다.    
# 효율적인 방법이 아니며, 내부 동작의 이해를 돕기 위해 만든 코드입니다.    
def updateDaysLastLogin(row):    
    parsed = row.asDict()  
    parsed['days_last_login'] = parsed['days_last_login'] +  missing_days    

    return Row(**parsed)

updated = list(map(updateDaysLastLogin, collected))

# updated[0] 의 출력 결과
Row(id=7196, year_birth=1950, education='PhD', count_kid=1, count_teen=1, date_customer='08-02-2014', days_last_login=30, date_joined=datetime.date(2020, 2, 8))
```

\
`missing_days = 10` 는 Python, 즉 메인 로직에만 (Driver) 존재하는 변수이며 이 값을 이용해 `days_last_login` 값을  변경한 것을 알 수 있습니다. 주석에 달린바와 같이 일반적으로는 위와 같은 방식으로 데이터를 가공하지 않습니다. Spark DataFrame 에서 `withColumn` 등을 통해 효율적으로 Executor 에서 분산처리 하는것이 효율적입니다.

\
Python 의 경우에는 `toPandas()` Action 을 이용하면 [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) 으로 바꾼 후 데이터를 Pandas API 로 변경할 수 있습니다. ([Pandas.Dataframe.add](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.add.html))\


{% hint style="info" %}
10개 의 Executor 가 나누어 처리하던 총합 100 GB 만큼의 데이터를, Driver 로 가져오려면 어떤일이 발생할까요? Driver 의 메모리가 모자라진 않을지, 데이터를 전송하는 과정에서 네트워크 비용이 비싸진 않을지 생각해봅시다. 

Q. 그렇다면 언제 Driver 로 데이터를 가져와야 할까요?
{% endhint %}

\
Driver 는 다음 설정을 통해 리소스를 조절할 수 있습니다. 만약 필요에 의해 `collect()` 등을 사용해 Driver 로 데이터를 가져와 처리한다면 리소스를 처리에 요구되는 만큼 늘려 사용할 수 있습니다.

* [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)

```
spark.driver.cores # Driver 에서 사용할 CPU Core 숫자
spark.driver.memory # Driver 에서 사용할 메모리 GiB
```

\


### Executor

\
**Executor** 는 Spark Driver 에서 요청한 작업을 분산처리 하거나 cache() 로 데이터를 분산 저장한 값을 들고 있습니다. 사용자 요청에 따라 갯수와 리소스를 조절할 수 있으며, 다음의 옵션을 사용합니다.

* [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)

```
spark.executor.instances # 하나의 Spark 작업에서 사용할 Executor 수
spark.executor.cores # 개별 Executor 에서 사용할 CPU Core 숫자
spark.executor.memory # 개별 Executor 에서 사용할 Memory GiB
```

\
`spark.executor.instances` 에 지정된 숫자 만큼 Executor 가 생성됩니다. Driver 는 Executor 를 기다리기 위해 아래에 지정된 `spark.scheduler` 옵션만큼 대기합니다. 80% 의 Executor 가 사용할 수 있는 상태가 되기까지 기다리거나, 아니면 30초가 넘을 경우 Task 를 할당합니다.

```
spark.scheduler.maxRegisteredResourcesWaitingTime = 30s (default)
spark.scheduler.minRegisteredResourcesRatio = 0.8 (for Kuberntes, Yarn)
```

따라서 Kubernetes 처럼 [Cluster Autoscaler](https://github.com/kubernetes/autoscaler) 로 인해 EC2 가 생성되고, 그 이후에 필요한 Docker Image 를 다운받는 등 대기 시간이 길 경우에는 위 옵션을 조절하면 충분한 시간만큼 대기 후 Executor 가 전부 준비가 되었을때 Task 를 할당해, 데이터가 특정 Executor 로 초기에 몰리는 것을 방지할 수 있습니다.

\
Executor 에서 분산 처리된 DataFrame 의 데이터를 Driver 에 데이터를 다시 모으기 위해 collect() 를 사용하는 방법을 다루었는데, 만약 Driver 로 데이터를 모을 필요가 없고, Executor 에서 그대로 데이터를 처리하고 싶다면 [foreach()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.foreach.html), [foreachPartition()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.foreachPartition.html?highlight=foreachpartition#pyspark.sql.DataFrame.foreachPartition) 함수를 사용할 수 있습니다.

```python
dfPartitioned = dfConverted.repartition(5)

# foreach 함수는 Callback 파라미터 함수로 Spark.DataFrame 의 Row 를 '하나씩' 전달합니다.
def persist(row):
    parsed = row.asDict()
    # Do something here
    
df.foreach(persist)
```

\
\
\
이 때 `foreach()` 로 넘겨주는 커스텀으로 만든 `persist` Callback 함수는 Driver 가 아닌 Executor 에서 동작합니다. 따라서 `persist()` 내에서 `print` 함수를 사용하더라도 Jupyter Notebook 이나 Driver 에서 결과가 출력되지 않습니다.

\
Driver 가 아닌 Executor Process 의 로그를 확인해보면 다음과 같은 출력을 확인할 수 있습니다.

```
... (생략)
Row(id=8397, year_birth=1951, education='Graduation', count_kid=1, count_teen=1, date_customer='10-01-2014', days_last_login=82, date_joined=datetime.date(2020, 1, 10))
Row(id=1685, year_birth=1967, education='PhD', count_kid=0, count_teen=0, date_customer='17-03-2013', days_last_login=21, date_joined=datetime.date(2019, 3, 17))
Row(id=5186, year_birth=1955, education='PhD', count_kid=0, count_teen=1, date_customer='12-03-2014', days_last_login=59, date_joined=datetime.date(2020, 3, 12))
Row(id=5429, year_birth=1948, education='PhD', count_kid=0, count_teen=1, date_customer='20-08-2013', days_last_login=10, date_joined=datetime.date(2019, 8, 20))
```

`foreach` 함수는 `Dataframe.write` 가 지원되지 않는 경우 혹은 더 세밀히 조절하기 위한 경우에 사용할 수 있는 Row-level API 로 일반적으로 [DynamoDB](https://aws.amazon.com/ko/dynamodb/) 등 외부 스토리지로 데이터를 내보내기 위해 사용할 수 있습니다.\
`DataFrame.write` 함수는 추후에 다루겠지만, 다양한 형식으로 데이터를 내보낼때 사용합니다. [Spark Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html) 문서에서 빌트인으로 저장 가능한 형식을 확인할 수 있습니다.

 [spark-redis](https://github.com/RedisLabs/spark-redis/blob/master/doc/dataframe.md) 처럼 벤더가 `DataFrame.write` 를 위한 라이브러리를 관리하는 경우도 있습니다. 아래에 몇개 스토리지에 대해 `DataFrame.write` 사용 가능한 경우를 정리해 봤습니다. 

* [Kafka Dataframe.write](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html) (Spark Docs)
* [ElasticSearch DataFrame.write](https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql-write) (ElasticSearch Docs)
* [Cassandra DataFrame.write](https://github.com/datastax/spark-cassandra-connector) (Datastax Github)

\
`foreach()` 함수가 DataFrame.Row (한 줄) 만큼 받아 처리할 수 있는 옵션을 제공한다면 `foreachPartition()` 함수는 Partition 단위로 데이터를 다룰 수 있는 Action 입니다. 우리는 이미 `DataFrame.repartition()` 함수를 통해 파티션을 적절히 나누어 놓았기 때문에 Partition 숫자만큼 `foreachPartition()` callback 이 호출됩니다.

```python
def persistPartition(partitionedRows):
    for row in partitionedRows:
        parsed = row.asDict()
        # DO something here
        
dfPartitioned.foreachPartition(persistPartition)
```

foreachPartition 은 데이터를 뭉텅이로, 즉 Partition 단위로 사용자에게 줍니다. 따라서 사용자는 외부 저장소로 보낼때 묶음 단위 로 보낼 수 있고 외부 저장소로 보내는 커넥션 비용이 비싼 경우에 `foreach()` 보다 `foreachPartition` 을 유용하게 사용할 수 있습니다.



### Cluster Manager

![Spark Cluster Mode Overview (https://spark.apache.org/docs/latest/cluster-overview.html) ](../../.gitbook/assets/image.png)



Cluster Manager 는 Spark 가 실행될 수 있는 리소스를 제공합니다. Driver 가 관리하는 SparkContext 는 어떤 Cluster Manager 에 연결할지에 대한 정보를 포함해 다양한 내용을 담고 있습니다.

Driver 를 통해 Spark 가 Cluster Manager 에 연결되면 Executor 실행에 필요한 리소스를 얻어올 수 있습니다. 

Spark 는 다양한 Cluster Manager 를 지원합니다.

* Standalone
* Apache Mesos
* Hadoop Yarn
* Kubernetes

이 일반적으로 Yarn 과 Kubernetes 가 많이 쓰입니다. AWS 환경에서는 [AWS EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html) 을 통해 Spark 를 위한 Yarn 클러스터를  쉽게 세팅할 수 있습니다.

Kubernetes 는 Spark 3.1+ 부터 GA 로 지원되는 Cluster Manager 입니다. Kubernetes 에 존재하는 [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) Addon 이나 [Prometheus Stack](https://github.com/prometheus-operator/kube-prometheus) 등 모니터링 레이어를 그대로 활용할 수 있습니다.



### Execution Environment







![Spark Local Mode (출 표기)](<../../.gitbook/assets/image (3).png>)



![Spark Client Mode (출처 표기)](<../../.gitbook/assets/image (4).png>)





![Spark Cluster Mode (출 표기)](<../../.gitbook/assets/image (5).png>)







