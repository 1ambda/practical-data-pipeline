# 2.1.2 Spark Concept

이해를 돕기 위해 Spark 의 구조에 대해 "간략화" 하여 설명합니다. 각각의 주제에 대한 자세한 설명은 본 문서의 개별 챕터 또는 공식 문서를 통해 확인할 수 있습니다. 



Spark 를 활용해 사용자는 데이터를 읽고 가공 할 수 있습니다. 예를 들어, 아래의 코드는 S3 에 있는 작은 CSV 파일을을 읽어 몇 가지 함수를 이용해 가공하는 샘플입니다. 

환경은 편의에 따라 Databricks Notebook / 로컬 내 PySpark Shell 등 자유롭게 사용할 수 있습니다. 이 문서에서는 코드를 간략히 표현하기 위해 PySpark 3 를 사용했습니다. 환경을 구축하는 부분은 Spark Installation 챕터 내에서 확인할 수 있습니다.

```scala
# 현재 디렉토리에 CSV 파일을 다운받은 후 아래 코드를 실행합니다.
# 해당 파일의 확장자는 `.csv` 로 되어있으나, 실제로 데이터의 구분자는 `\t` (탭) 입니다

df = spark.read.load("./marketing_campaign.csv",
                     format="csv", sep="\t", inferSchema="true", header="true")

```

위 코드에서 사용한 데이터셋은 [Kaggle: Customer Personality Analysis](https://www.kaggle.com/imakash3011/customer-personality-analysis) 입니다.



작은 데이터 파일을 읽어, Spark 가 메모리에 





RDD

Partition



Sark



Transformation

Action

Task

Stage

Driver Worker
